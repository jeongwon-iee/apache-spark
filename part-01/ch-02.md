# Part 1. 빅데이터와 스파크 간단히 살펴보기

### 목차

- Chapter 2. 스파크 간단히 살펴보기
  - 2.1 스파크의 기본 아키텍처
  - 2.2 스파크의 다양한 언어 API
  - 2.3 스파크의 API
  - 2.4 스파크 시작하기
  - 2.5 SparkSession
  - 2.6 DataFrame
  - 2.7 트랜스포메이션
  - 2.8 액션
  - 2.9 스파크 UI
  - 2.10 종합 예제
  - 2.11 정리
 
&nbsp;

## Chapter 2. 스파크 간단히 살펴보기

1장에서는 아파치 스파크의 역사를 살펴보았다. 이제 스파크를 사용해보려고 한다.  

이 장에서는 DataFrame과 SQL을 사용해 클러스터, 스파크 애플리케이션 그리고 구조적 API를 살펴볼 것이다.  
이 과정에서 **스파크의 핵심 용어와 개념**을 접하고 사용법을 익힐 것이다.

&nbsp;

### 2.1 스파크의 기본 아키텍처

#### 대용량 데이터 처리를 위한 클러스터와 클러스터를 관리하는 프레임워크 Spark

한 대의 컴퓨터(집이나 직장 책상 위에 놓인 장비 한 대)로는 대규모 정보를 연산할 만한 자원이나 성능을 가지지 못해 데이터 처리가 힘들다.  

컴퓨터 **클러스터**는 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게 만든다.  
하지만 컴퓨터 클러스터를 구성하는 것만으로는 부족, 클러스터에서 작업을 조율할 수 있는 프레임워크가 필요하다.  
스파크가 이런 역할을 하는 프레임워크다.

**스파크는 클러스터의 데이터 처리 작업을 관리하고 조율한다.**  
스파크가 연산에 사용하는 클러스터는 스파크 standalone 클러스터 매니저, 하둡 YARN, Mesos 같은 *클러스터 매니저* 에서 관리한다.  

사용자는 클러스터 매니저에 스파크 애플리케이션을 제출(submit)하고,  
이를 제출 받은 클러스터 매니저는 애플리케이션 실행에 필요한 자원을 할당하며, 우리는 할당받은 자원으로 작업을 처리한다.

#### 2.1.1 스파크 애플리케이션

스파크 애플리케이션은 **드라이버**(driver) 프로세스와 다수의 **익스큐터**(executor) 프로세스로 구성된다.  
- 드라이버 프로세스
  - 클러스터 노드 중 하나에서 실행되며, `main()` 함수를 실행한다.
  - 스파크 애플리케이션 정보의 유지 관리
  - 사용자 프로그램이나 입력에 대한 응답
  - 전반적인 익스큐터 프로세스의 작업과 관련된 분석, 배포, 그리고 스케쥴링 역할을 수행
  - 애플리케이션의 수명 주기 동안 관련 정보를 모두 유지
- 익스큐터 프로세스
  - 드라이버 프로세스가 할당한 작업을 수행한다.
  - 드라이버가 할당한 코드를 실행하고, 진행 상황을 다시 드라이버 노드에 보고하는 두 가지 역할을 수행한다.

<img width="428" alt="스크린샷 2022-01-23 오후 5 37 32" src="https://user-images.githubusercontent.com/45806836/150670862-d0ffe99d-c932-4a72-ba5c-f7c708dc54bb.png">

위 그림은 클러스터 매니저가 물리적 머신을 관리하고 스파크 애플리케이션에 자원을 할당하는 방법을 나타낸다.

클러스터 매니저는 스파크 standalone 클러스터 매니저, 하둡 YARN, 혹은 Mesos 중 하나를 선택할 수 있으며,  
하나의 클러스터에서 여러 개의 스파크 애플리케이션을 실행할 수 있다.  
사용자는 각 노드에 할당할 익스큐터 수를 지정할 수 있다.

*참고*  
스파크는 클러스터 모드뿐 아니라 **로컬 모드**도 지원한다.  
드라이버와 익스큐터는 단순한 프로세스이므로 같은 머신이나 서로 다른 머신에서 실행할 수 있다.  
하지만 로컬 모드로 실행하면 드라이버와 익스큐터를 단일 머신에서 스레드 형태로 실행한다.

#### 정리

- 스파크는 사용 가능한 자원을 파악하기 위해 클러스터 매니저를 사용한다.
- 드라이버 프로세스는 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 익스큐터 프로세스에서 실행한다.

&nbsp;

### 2.2 스파크의 다양한 언어 API

스파크의 언어 API를 사용하면 다양한 프로그래밍 언어로 스파크 코드를 실행할 수 있다.  
스파크는 모든 언어에 맞는 몇몇 '핵심 개념'을 제공하며, 이러한 핵심 개념은 클러스터 머신에서 실행되는 스파크 코드로 변환된다.  
구조적 API만으로 작성된 코드는 언어에 사오간없이 유사한 성능을 발휘한다.

#### 스칼라

스파크는 스칼라로 개발되어 있으므로 스칼라가 스파크의 기본 언어다.

#### 자바

스파크가 스칼라로 개발되어 있지만, 스파크 창시자들은 자바를 이용해 스파크 코드를 작성할 수 있도록 심혈을 기울였다고 한다.

#### SQL

스파크는 ANSI SQL:2003 표준 중 일부를 지원한다.

#### SparkSession과 스파크 언어 API의 관계

<img width="555" alt="스크린샷 2022-01-23 오후 6 22 33" src="https://user-images.githubusercontent.com/45806836/150672231-cb59e9b3-c96b-49f4-91ef-a5948b33227b.png">

각 언어 API는 핵심 개념을 유지한다. 사용자는 스파크 코드를 실행하기 위해 `SparkSession` 객체를 진입점으로 사용한다.  
스파크는 사용자를 대신해 파이썬이나 R로 작성한 코드를 익스큐터의 JVM에서 실행할 수 있는 코드로 변환한다.

&nbsp;

### 2.3 스파크 API

다양한 언어로 스파크를 사용할 수 있는 이유는 스파크가 기본적으로 두 가지 API를 제공하기 때문이다.

하나는 저수준의 비구조적(unstructured) API이며, 다른 하나는 고수준의 구조적(structured) API이다.

&nbsp;

### 2.4 스파크 시작하기

실제 스파크 애플리케이션을 개발하려면 사용자 명령과 데이터를 스파크 애플리케이션에 전송하는 방법을 알아야 한다.

&nbsp;

### 2.5 SparkSession

대화형 모드(`./bin/spark-shell`)로 스파크를 시작하면 스파크 애플리케이션을 관리하는 SparkSession이 자동으로 생성된다.  
만약 스탠드얼론 애플리케이션으로 스파크를 시작하면 사용자 애플리케이션 코드에서 SparkSession 객체를 직접 생성해야 한다.

> 스파크 애플리케이션은 **SparkSession**이라는 드라이버 프로세스로 제어한다.  
SparkSession 인스턴스는 사용자가 정의한 처리 명령을 클러스터에서 실행한다.

하나의 SparkSession은 하나의 스파크 애플리케이션에 대응한다.

스칼라나 파이썬에서 SparkSession을 확인하기 위해 `spark`를 입력할 수 있다.

```
./bin/spark-shell

Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/01/23 18:31:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://192.168.0.102:4040
Spark context available as 'sc' (master = local[*], app id = local-1642930300205).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.13)
Type in expressions to have them evaluated.
Type :help for more information.


scala> spark
res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@9c53820
```

&nbsp;

### 2.6 DataFrame

```scala
scala> var myRange = spark.range(1000).toDF("number")
myRange: org.apache.spark.sql.DataFrame = [number: bigint]
```

위의 코드는 한 개의 column과 1,000개의 row로 구성되어 각 row에 0부터 999까지의 값이 할당된 DataFrame을 생성한다.  
이 숫자들은 **분산 컬렉션**을 나타낸다.  
클러스터 모드에서 위 코드를 실행하면 숫자 범위(0~999)의 각 부분이 서로 다른 익스큐터에 할당된다. 이것이 스파크의 DataFrame이다.

DataFrame은 가장 대표적인 구조적 API이다.  
DataFrame은 테이블의 데이터를 로우와 컬럼으로 표현한다.  
컬럼과 컬럼의 타입을 정의한 목록은 스키마(schema)라고 부른다.  
DataFrame은 컬럼에 이름을 붙인 스프레드시트와 비슷한데,  
둘의 근본적 차이는 스프레드시트는 한 대의 컴퓨터에 있지만, **스파크의 DataFrame은 수천 대의 컴퓨터에 분산되어 있다**는 것이다.  
여러 대의 컴퓨터에 데이터를 분산하는 이유는 단순하다.  
단일 컴퓨터에 저장하기에는 데이터가 너무 크거나 계산에 너무 오랜 시간이 걸릴 수 있기 때문이다.

파이썬과 R에도 DataFrame이라는 개념이 존재하는데, '일반적으로' 분산 컴퓨터가 아닌 단일 컴퓨터상에 존재한다.  
이런 상황에서는 DataFrame으로 수행할 수 있는 작업이 해당 머신이 가진 자원에 따라 제한될 수 밖에 없다.  
스파크는 파이썬과 R언어를 모두 지원하기 때문에 파이썬 Pandas 라이브러리의 DataFrame과 R이 DataFrame을 스파크 DataFrame으로 쉽게 변환할 수 있다.

*참고*  
스파크는 Dataset, DataFrame, SQL 테이블 그리고 RDD라는 핵심 추상화 개념을 가진다. 이 개념 모두 분산 데이터 모음을 표현한다.

&nbsp;

#### 2.6.1 파티션

스파크는 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 **파티션**이라 불리는 청크 단위로 데이터를 분할한다.

파티션은 **클러스터의 물리적 머신에 존재하는 로우의 집합**을 의미한다.  
DataFrame의 파티션은 실행 중에 데이터가 컴퓨터 클러스터에서 물리적으로 분산되는 방식을 나타낸다. 

만약 파티션이 하나라면, 스파크에 수천 개의 익스큐터가 있더라도 병렬성은 1이다.  
또한 수백 개의 파티션이 있더라도 익스큐터가 하나밖에 없다면 병렬성은 1이다.

DataFrame을 사용하면 파티션을 수동 혹은 개별적으로 처리할 필요가 없다.  
물리적 파티션에 데이터 변환용 함수를 지정하면 스파크가 실제 처리 방법을 결정한다.

&nbsp;

### 2.7 트랜스포메이션

스파크의 핵심 데이터 구조는 **불변성**(Immutable)을 가진다. 즉, 한 번 생성하면 변경할 수 없다.  

이상한 개념처럼 보인다. 데이터 구조를 변경할 수 없다면 어떻게 사용해야 할까?  
DataFrame을 '변경'하려면 원하는 변경 방법을 스파크에 알려줘야 한다.  
이 때 사용하는 명령을 **트랜스포메이션**이라고 부른다.

다음은 DataFrame에서 짝수를 찾는 간단한 트랜스포메이션 예제다.

```scala
val myRange = spark.range(1000).toDF("number")
val divisBy2 = myRange.where("number % 2 = 0")
```

위 코드를 실행해도 결과는 출력되지 않으며(응답값은 출력된다),  
추상적인 트랜스포메이션만 지정한 상태이기 때문에 액션(action)을 호출하지 않으면 실제 트랜스포메이션을 수행하지 않는다.

트랜스포메이션은 스파크에서 비즈니스 로직을 표현하는 핵심 개념이다.  
트랜스포메이션에는 두 가지 유형이 있다.  
- 좁은 의존성(narrow dependency)
- 넓은 의존성(wide dependency)

<img width="552" alt="스크린샷 2022-01-23 오후 6 56 07" src="https://user-images.githubusercontent.com/45806836/150673266-1161fbaf-0433-4f36-b1a3-50021f008340.png">


#### 좁은 의존성

좁은 의존성을 가진 트랜스포메이션(좁은 트랜스포메이션)은 각 입력 파티션이 하나의 출력 파티션에만 영향을 미친다.  

이전 코드 예제에서 `where` 구문은 좁은 의존성을 가진다.  
따라서 하나의 파티션이 하나의 출력 파티션에만 영향을 미친다.

#### 넓은 의존성

넓은 의존성을 가진 트랜스포메이션(넓은 트랜스포메이션)은 하나의 입력 파티션이 여러 출력 파티션에 영향을 미친다.  

좁은 트랜스포메이션을 사용하면 스파크에서 자동으로 **파이프라이닝**을 수행한다.  
즉, DataFrame에 여러 필터를 지정하는 경우 모든 작업이 메모리에서 일어난다.

하지만, 스파크가 클러스터에서 파티션을 교환하는 **셔플**(shuffle)을 사용하는 넓은 트랜스포메이션은 셔플의 결과를 디스크에 저장한다.  
이 내용은 사실 아주 중요하다. (셔플 최적화)

#### 2.7.1 지연 연산

지연 연산(lazy evaluation)은 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미한다.

스파크는 특정 연산 명령이 내려진 즉시 데이터를 수정하지 않고, 원시 데이터에 적용할 트랜스포메이션의 **실행 계획**을 생성한다.  
스파크는 코드를 실행하는 마지막 순간까지 대기하다가 원형 DataFrame 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일한다.

DataFrame의 **조건절 푸시다운**(predicate pushdown)이 한 예다.  
아주 복잡한 스파크 잡이 원시 데이터에서 하나의 로우만 가져오는 필터를 가지고 있다면 필요한 레코드 하나만 읽는 것이 가장 효율적이다.  
스파크는 이 필터를 데이터소스로 위임하는 최적화 작업을 자동으로 수행한다.   
만약 데이터 저장소가 DB라면 WHERE 절의 처리를 DB에 위임하고, 스파크는 하나의 레코드만 받는 것이다.  
조건절 위임을 사용하면 처리에 필요한 자원을 최소화할 수 있다.

&nbsp;

### 2.8 액션

사용자는 트랜스포메이션을 사용해 논리적 실행 계획을 세울 수 있지만, 실제 연산을 수행하려면 **액션** 명령을 내려야 한다.  
액션은 일련의 트랜스포메이션으로부터 **결과를 계산하도록 지시하는 명령**이다.

가장 단순한 액션인 `count` 메서드는 `DataFrame`의 전체 레코드 수를 반환한다.

```scala
val myRange = spark.range(1000).toDF("number")
val divisBy2 = myRange.where("number % 2 = 0")
divisBy2.count()

res1: Long = 500
```

위 코드는 500을 출력한다. `count` 외에도 다음 세 가지 유형의 액션이 있다.  

- 콘솔에서 데이터를 보는 액션
- 각 언어로 된 네이티브 객체에 데이터를 모으는 액션
- 출력 데이터소스에 저장하는 액션

액션을 지정하면 스파크 잡(job)이 시작된다.  
스파크 잡은 필터(좁은 트랜스포메이션)를 수행한 후 파티션별로 레코드 수를 카운트(넓은 트랜스포메이션)한다.  
그리고 각 언어에 적합한 네이티브 객체에 결과를 모은다. 이 때 스파크가 제공하는 스파크 UI로 클러스터에서 실행 중인 스파크 잡을 모니터링할 수 있다.

&nbsp;

### 2.9 스파크 UI

스파크 UI는 스파크 잡의 진행 상황을 모니터링할 때 사용한다. 스파크 UI는 드라이버 노드의 4040 포트로 접속할 수 있다.  
로컬 모드에서 스파크를 실행했다면 스파크 UI의 주소는 `http://localhost:4040`이다.

<img width="1312" alt="스크린샷 2022-01-23 오후 7 19 22" src="https://user-images.githubusercontent.com/45806836/150674015-79a0a97d-a1a5-464b-aa67-b1e3f98f9059.png">

스파크 UI에서 스파크 잡의 상태, 환경 설정, 클러스터 상태 등의 정보를 확인할 수 있다.  

<img width="1312" alt="스크린샷 2022-01-23 오후 7 21 39" src="https://user-images.githubusercontent.com/45806836/150674095-9155b346-cfe2-4961-ae16-fa4daf130199.png">

로컬 모드에서 스파크를 실행했으므로, 익스큐터의 개수는 당연히 한 개인 것을 볼 수 있다.

#### 기억할 것

> 스파크 잡은 개별 액션에 의해 트리거되는 다수의 트랜스포메이션으로 이루어져 있으며 스파크 UI로 잡을 모니터링 할 수 있다.

&nbsp;

### 2.10 종합 예제

이전 예제에서는 비록 빅데이터는 아니지만 일련의 숫자로 구성된 DataFrame을 생성해보았다. 

이제 현실적인 예제로 지금까지 익힌 내용을 확실히 이해하고 스파크 내부에서 어떤 일이 일어나는지 단계별로 살펴볼 것이다.  
[미국 교통 통계국의 항공운항 데이터](https://github.com/FVBros/Spark-The-Definitive-Guide/tree/master/data/flight-data/csv) 중 일부를 스파크로 분석할 것이다.

예제용 CSV 디렉터리에는 데이터 파일이 여럿 있다. 일단 지금은 CSV 포맷의 파일만 사용할 것이다.

각 파일은 여러 로우를 가진다. 이 파일들은 csv 파일이며 반정형(semi-structured) 데이터 포맷이다. 파일의 각 로우는 DataFrame의 로우가 된다.  

```
head 2015-summary.csv 
DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count
United States,Romania,15
United States,Croatia,1
United States,Ireland,344
Egypt,United States,15
United States,India,62
United States,Singapore,1
United States,Grenada,62
Costa Rica,United States,588
Senegal,United States,40
```

스파크는 다양한 데이터소스를 지원하는데, 데이터는 SparkSession의 `DataFrameReader` 클래스를 사용해 읽는다.  
이 때 특정 파일 포맷과 몇 가지 옵션을 함께 설정한다.  
예제에서는 스파크 DataFrame의 스키마 정보를 알아내는 **스키마 추론**(schema inference) 기능을 사용한다.  
그리고 파일의 첫 로우를 헤더로 지정하는 옵션도 함께 설정한다.

스파크는 스키마 정보를 얻기 위해 데이터를 조금 읽는다. 그리고 해당 로우의 데이터 타입을 스파크 데이터 타입에 맞게 분석한다.  
(하지만 운영 환경에서는 데이터를 읽는 시점에 스키마를 엄격하게 지정하는 옵션을 사용해야 한다.)

```scala
val flightData2015 = spark
  .read
  .option("inferSchema", "true")
  .option("header", "true")
  .csv("/Users/user/NAVER/spark/Spark-The-Definitive-Guide/data/flight-data/csv")
  
flightData2015: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]

```

스칼라에서 사용하는 DataFrame은 불특정 다수의 로우와 컬럼을 가진다.  
로우의 수를 알 수 없는 이유는 데이터를 읽는 과정이 지연 연산 형태의 트랜스포메이션이기 때문이다.

스파크는 각 컬럼의 데이터 타입을 추론하기 위해 적은 양의 데이터를 읽고, DataFrame에서 CSV 파일을 읽어 로컬 배열이나 리스트 형태로 변환한다.

DataFrame의 `take` 액션을 호출하면 이전의 `head` 명령과 같은 결과를 확인할 수 있다.

```scala
scala> flightData2015.take(3)

res5: Array[org.apache.spark.sql.Row] = Array([United States,Romania,1], [United States,Ireland,264], [United States,India,69])
```

이제 트랜스포메이션을 추가로 지정하려고 한다. 정수 데이터 타입인 *count* 컬럼을 기준으로 데이터를 정렬한다. 

*참고*  
`sort` 메서드는 DataFrame을 변경하지 않는다. 트랜스포메이션으로 `sort` 메서드를 사용하면 이전의 DataFrame을 변환해 새로운 DataFrame을 생성해 반환한다.  

`sort` 메서드는 단지 트랜스포메이션이기 때문에 호출 시 데이터에 아무런 변화도 일어나지 않는다.  
하지만 스파크는 실행 계획을 만들고 검토하여 클러스터에서 처리할 방법을 알아낸다.  
특정 DataFrame 객체에 `explain` 메서드를 호출하면 DataFrame의 계보(lineage)나 스파크의 쿼리 **실행 계획**을 확인할 수 있다.

```scala
scala> flightData2015.sort("count").explain()
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Sort [count#28 ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning(count#28 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#74]
      +- FileScan csv [DEST_COUNTRY_NAME#26,ORIGIN_COUNTRY_NAME#27,count#28] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/user/.../spark/Spark-The-Definitive-Guide/data/flight-da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>
```

실행 계획은 다소 어려워 보이지만 조금만 연습하면 무리 없이 이해할 수 있다.  
실행 계획은 위에서 아래 방향으로 읽으며, 최종 결과(`Sort`)는 가장 위에, 데이터 소스(`FileScan`)는 가장 아래에 있다.  
위 예제에서는 각 줄의 첫 번째 키워드(`Sort`, `Exchange`, `FileScan`)를 주목한다.  
특정 컬럼을 다른 컬럼과 비교하는 `sort` 메서드가 넓은 트랜스포메이션으로 동작하는 것을 볼 수 있다.

> 실행 계획은 디버깅과 스파크의 실행 과정을 이해하는 데 도움을 주는 도구이다.

이제 트랜스포메이션 실행 계획을 시작하기 위해 액션을 호출한다.  
액션을 실행하려면 몇 가지 설정이 필요하다. 스파크는 셔플 수행 시 기본적으로 200개의 셔플 파티션을 생성한다.  
이 값을 5로 설정해 셔플의 출력 파티션 수를 줄일 것이다.

```scala
scala> spark.conf.set("spark.sql.shuffle.partitions", "5")

scala> flightData2015.sort("count").take(2)
res9: Array[org.apache.spark.sql.Row] = Array([United States,Croatia,1], [Malta,United States,1])
```

트랜스포메이션의 논리적 실행 계획은 DataFrame의 계보를 정의한다.  
스파크는 계보를 통해 입력 데이터에 수행한 연산을 전체 파티션에서 어떻게 재연산하는지 알 수 있다.  
이 기능은 스파크의 프로그래밍 모델인 함수형 프로그래밍의 핵심이다.  
함수형 프로그래밍은 데이터의 변환 규칙이 일정한 경우 같은 입력에 대해 항상 같은 출력을 생성한다.

사용자는 물리적 데이터를 직접 다루지 않는 대신, 앞서 설정한 셔플 파티션 파라미터와 같은 속성으로 물리적 실행 특성을 제어한다.  
앞서 셔플 파티션 수를 5로 설정했기 때문에 5개의 출력 파티션이 생성된다. 이 값을 변경하면 스파크 잡의 실제 실행 특성을 제어할 수 있다.  
값을 변경하면 런타임이 크게 달라질 수 있다. 그리고 스파크 UI에 접속해 잡의 실행 상태와 스파크 잡의 물리적, 논리적 실행 특성을 확인할 수 있다.

#### 2.10.1 DataFrame과 SQL

이전 예제에서는 간단한 트랜스포메이션 작업을 수행해봤다.

이어서 DataFrame과 SQL을 사용하는 복잡한 작업을 수행해보겠다.  

스파크는 언어에 상관없이 같은 방식으로 트랜스포메이션을 실행한다.  
사용자가 SQL이나 DataFrame으로 비즈니스 로직을 표현하면 스파크에서 실제 코드를 실행하기 전에  
그 로직을 기본 실행 계획(`explain` 메서드를 호출하면 확인할 수 있는 실행 계획)으로 컴파일 한다.

스파크 SQL을 사용하면 모든 DataFrame을 테이블이나 뷰(임시 테이블)로 등록한 후 SQL 쿼리를 사용할 수 있다.  
스파크는 SQL 쿼리를 DataFrame 코드와 같은 실행 계획으로 컴파일하므로 둘 사이의 성능 차이는 없다.

아래 코드는 `createOrReplaceTempView` 메서드를 호출해 모든 DataFrame을 테이블이나 뷰로 만드는 예제이다.

```scala
flightData2015.createOrReplaceTempView("flight_data_2015")
```

이제 SQL로 데이터를 조회할 수 있다.  
DataFrame에 `spark.sql` 메서드로 SQL 쿼리를 실행하면 새로운 DataFrame이 반환된다.  
`spark`는 SparkSession의 변수이다.

SQL 쿼리와 DataFrame 코드의 성능을 비교하기 위해 아래와 같이 두 가지 방법으로 코드를 작성해 보겠다.

```scala
scala> val sqlWay = spark.sql("""
  SELECT DEST_COUNTRY_NAME, count(1) 
  FROM flight_data_2015 
  GROUP BY DEST_COUNTRY_NAME
  """)

sqlWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count(1): bigint]

scala> val dataFrameWay = flightData2015
  .groupBy("DEST_COUNTRY_NAME")
  .count()

dataFrameWay: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, count: bigint]

scala> sqlWay.explain

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[count(1)])
   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#26, 5), ENSURE_REQUIREMENTS, [id=#96]
      +- HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[partial_count(1)])
         +- FileScan csv [DEST_COUNTRY_NAME#26] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/user/NAVER/spark/Spark-The-Definitive-Guide/data/flight-da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>

scala> dataFrameWay.explain

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[count(1)])
   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#26, 5), ENSURE_REQUIREMENTS, [id=#109]
      +- HashAggregate(keys=[DEST_COUNTRY_NAME#26], functions=[partial_count(1)])
         +- FileScan csv [DEST_COUNTRY_NAME#26] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/user/NAVER/spark/Spark-The-Definitive-Guide/data/flight-da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>
```

두 가지 실행 계획은 동일한 기본 실행 계획으로 컴파일 되는 것을 알 수 있다. 

